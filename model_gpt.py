from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­Ø§Ø¯Ø«Ø© ØµØºÙŠØ± Ù…Ù† Ù…Ø§ÙŠÙƒØ±ÙˆØ³ÙˆÙØª
model_name = "microsoft/DialoGPT-small"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

chat_history_ids = None  # Ù„Ø­ÙØ¸ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ù‚ØµÙŠØ±Ø©

def generate_bot_message(prompt: str, max_length: int = 100) -> str:
    """
    ØªÙˆÙ„ÙŠØ¯ Ø±Ø¯ Ø°ÙƒÙŠ ÙˆØ³Ø±ÙŠØ¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… DialoGPT-small
    """
    global chat_history_ids

    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØªÙˆÙƒÙ†Ø²
    new_input_ids = tokenizer.encode(prompt + tokenizer.eos_token, return_tensors="pt")

    # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯ Ø§Ø¹ØªÙ…Ø§Ø¯Ø§Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ§Ø±ÙŠØ®
    bot_output = model.generate(
        torch.cat([chat_history_ids, new_input_ids], dim=-1) if chat_history_ids is not None else new_input_ids,
        max_length=max_length,
        pad_token_id=tokenizer.eos_token_id,
        do_sample=True,
        top_p=0.9,
        temperature=0.8,
    )

    chat_history_ids = bot_output  # Ø­ÙØ¸ Ø§Ù„ØªØ§Ø±ÙŠØ® Ù„Ù„Ø±Ø¯ Ø§Ù„ØªØ§Ù„ÙŠ

    reply = tokenizer.decode(bot_output[:, new_input_ids.shape[-1]:][0], skip_special_tokens=True)

    # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø±Ø¯ Ù‚Ù„ÙŠÙ„Ø§Ù‹
    if reply.strip() == "":
        reply = "Ø£Ù†Ø§ Ù‡Ù†Ø§ Ø£Ø³Ø§Ø¹Ø¯Ùƒ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª ğŸ¬ØŒ Ø£Ø±Ø³Ù„ Ø§Ù„Ø±Ø§Ø¨Ø· ÙÙ‚Ø·!"
    elif len(reply) > 200:
        reply = reply[:200] + "..."

    return reply
